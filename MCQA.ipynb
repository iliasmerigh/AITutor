{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOqRTo6fwJ+dnn7uh7cKJzt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"f24513bc8a8d4fecb033fb8c87642730":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c9f8cc96491f4e4ab01aef4f4c86480e","IPY_MODEL_3f075aea1b234aeeb966b9f2f154fa3f","IPY_MODEL_b3ce3a6ffcfe4e36a016681fb8184bd7"],"layout":"IPY_MODEL_6815d005925b4c9f82d91edf770d7849"}},"c9f8cc96491f4e4ab01aef4f4c86480e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d585b44d142540b78e2784a5f0068b74","placeholder":"​","style":"IPY_MODEL_58983789d4814cbf9ec99cee126bd7a8","value":"Map: 100%"}},"3f075aea1b234aeeb966b9f2f154fa3f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_30ead5a037af44bfa8fde541ef15e110","max":1218,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4c645ee6e5854b49b902747ecc3ed86d","value":1218}},"b3ce3a6ffcfe4e36a016681fb8184bd7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5da0a8663d09409eb3678f995a25b8b6","placeholder":"​","style":"IPY_MODEL_2d4c3d50c37f48488328efd929802c56","value":" 1218/1218 [00:05&lt;00:00, 141.10 examples/s]"}},"6815d005925b4c9f82d91edf770d7849":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d585b44d142540b78e2784a5f0068b74":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58983789d4814cbf9ec99cee126bd7a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"30ead5a037af44bfa8fde541ef15e110":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c645ee6e5854b49b902747ecc3ed86d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5da0a8663d09409eb3678f995a25b8b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d4c3d50c37f48488328efd929802c56":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e74a91be03c544ca9fdb1667e901cace":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c54354c1203a44b8b94c084ba4255bfa","IPY_MODEL_9e592fefb5144192a951ca54f3ed98e8","IPY_MODEL_2445c7ec787f469cac196e2013905a4d"],"layout":"IPY_MODEL_935aa0c468a64fb29cf5b1ffed009733"}},"c54354c1203a44b8b94c084ba4255bfa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3bd10c38a92a45c3b5302e85a06fb61a","placeholder":"​","style":"IPY_MODEL_17b727a8691a473997dad506c686cdd8","value":"Map: 100%"}},"9e592fefb5144192a951ca54f3ed98e8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e01e8d8ae0e4497ca690634475b62300","max":1218,"min":0,"orientation":"horizontal","style":"IPY_MODEL_de07a8066f01494889541d7a997fbdc8","value":1218}},"2445c7ec787f469cac196e2013905a4d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_453fd7a5ddbe4c21aa087704d3ec2428","placeholder":"​","style":"IPY_MODEL_f76b9bcb963747fd983dc262c7d01285","value":" 1218/1218 [00:03&lt;00:00, 487.22 examples/s]"}},"935aa0c468a64fb29cf5b1ffed009733":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bd10c38a92a45c3b5302e85a06fb61a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17b727a8691a473997dad506c686cdd8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e01e8d8ae0e4497ca690634475b62300":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de07a8066f01494889541d7a997fbdc8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"453fd7a5ddbe4c21aa087704d3ec2428":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f76b9bcb963747fd983dc262c7d01285":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AItcIQ4AvD0p","executionInfo":{"status":"ok","timestamp":1718055969119,"user_tz":-180,"elapsed":7737,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}},"outputId":"132d1f84-a947-4470-b42c-9d78aefe6c94"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n","Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.11.1)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.2)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.31.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.3.0+cu121)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.5.40)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"]}],"source":["!pip install transformers peft datasets accelerate -U"]},{"cell_type":"code","source":["# This cell makes sure modules are auto-loaded when you change external python files\n","%load_ext autoreload\n","%autoreload 2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_DCzsMMawVsy","executionInfo":{"status":"ok","timestamp":1718061119068,"user_tz":-180,"elapsed":278,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}},"outputId":"79e84133-8107-4d55-a3a4-21341562f83a"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}]},{"cell_type":"code","source":["# Step 1: Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hd80oVtOwWrS","executionInfo":{"status":"ok","timestamp":1718061123927,"user_tz":-180,"elapsed":2097,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}},"outputId":"46de6126-9309-42b5-b60f-80dc3eb9b92c"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Direct to your assignment folder.\n","%cd /content/drive/MyDrive/project-m2-2024-jim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z--8o_c9wYe6","executionInfo":{"status":"ok","timestamp":1718061125311,"user_tz":-180,"elapsed":274,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}},"outputId":"ec91983a-1f19-4169-97a2-668b70ac7660"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/project-m2-2024-jim\n"]}]},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","dataset = load_dataset('json', data_files='data/mcq_aquarat_dataset.jsonl')\n","\n","# Print dataset information\n","print(dataset)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fwF3PmPBwbu0","executionInfo":{"status":"ok","timestamp":1718061127326,"user_tz":-180,"elapsed":614,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}},"outputId":"1dc9d16d-1a4a-46ba-c4b7-0956b5ba4acb"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['subject', 'question', 'answer'],\n","        num_rows: 97467\n","    })\n","})\n"]}]},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","\n","# Shuffle the entire training dataset\n","shuffled_dataset = dataset['train'].shuffle(seed=42)\n","\n","# Define the size for the subsets (e.g., 10% for training, 10% for validation)\n","subset_size = len(dataset['train']) // 80\n","\n","# Generate the training subset (first 10%)\n","train_subset = shuffled_dataset.select(range(0, subset_size))\n","\n","# Generate the validation subset (next 10%)\n","validation_subset = shuffled_dataset.select(range(subset_size, 2 * subset_size))\n","\n","# Print the subset information\n","print(\"Training Subset:\")\n","print(train_subset)\n","\n","print(\"\\nValidation Subset:\")\n","print(validation_subset)\n","\n","#### Having a small training dataset and validation dataset\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3E0N1n1106c9","executionInfo":{"status":"ok","timestamp":1718061903686,"user_tz":-180,"elapsed":274,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}},"outputId":"6224c84c-b302-4f1a-cc4b-1a495b3496e3"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Subset:\n","Dataset({\n","    features: ['subject', 'question', 'answer'],\n","    num_rows: 1218\n","})\n","\n","Validation Subset:\n","Dataset({\n","    features: ['subject', 'question', 'answer'],\n","    num_rows: 1218\n","})\n"]}]},{"cell_type":"code","source":["#### ignore this\n","\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","model_output_dir = \"./model_output_second\"\n","#model = AutoModelForCausalLM.from_pretrained(model_output_dir)\n","#model.config.use_cache = False\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_output_dir)\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","def preprocess_data_1(example):\n","    inputs = tokenizer(example[\"question\"], padding=\"max_length\", truncation=True, max_length=1024, return_tensors=\"pt\")\n","    labels = tokenizer(example[\"answer\"], padding=\"max_length\", truncation=True, max_length=1024, return_tensors=\"pt\").input_ids\n","    labels[labels == tokenizer.pad_token_id] = -100  # Set padding tokens to -100 to ignore them in the loss\n","    return {\"input_ids\": inputs.input_ids.squeeze(), \"attention_mask\": inputs.attention_mask.squeeze(), \"labels\": labels.squeeze()}\n","\n","\n","def preprocess_data(example):\n","    # Tokenize and pad the question\n","    inputs = tokenizer(example[\"question\"], padding=\"max_length\", truncation=True, max_length=1024, return_tensors=\"pt\")\n","\n","    # Convert the single letter answer to an integer index\n","    label_map = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}  # Assuming the choices are A, B, C, and D\n","    label = label_map[example[\"answer\"].strip()]  # Convert label to integer\n","\n","    # Prepare the inputs and labels\n","    return {\n","        \"input_ids\": inputs.input_ids.squeeze(),\n","        \"attention_mask\": inputs.attention_mask.squeeze(),\n","        \"labels\": torch.tensor(label)\n","    }\n","# Apply preprocessing to the train subset\n","train_dataset = train_subset.map(preprocess_data_1, remove_columns=[\"subject\", \"question\", \"answer\"])\n","\n","validation_dataset = validation_subset.map(preprocess_data_1, remove_columns=[\"subject\", \"question\", \"answer\"])\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["f24513bc8a8d4fecb033fb8c87642730","c9f8cc96491f4e4ab01aef4f4c86480e","3f075aea1b234aeeb966b9f2f154fa3f","b3ce3a6ffcfe4e36a016681fb8184bd7","6815d005925b4c9f82d91edf770d7849","d585b44d142540b78e2784a5f0068b74","58983789d4814cbf9ec99cee126bd7a8","30ead5a037af44bfa8fde541ef15e110","4c645ee6e5854b49b902747ecc3ed86d","5da0a8663d09409eb3678f995a25b8b6","2d4c3d50c37f48488328efd929802c56","e74a91be03c544ca9fdb1667e901cace","c54354c1203a44b8b94c084ba4255bfa","9e592fefb5144192a951ca54f3ed98e8","2445c7ec787f469cac196e2013905a4d","935aa0c468a64fb29cf5b1ffed009733","3bd10c38a92a45c3b5302e85a06fb61a","17b727a8691a473997dad506c686cdd8","e01e8d8ae0e4497ca690634475b62300","de07a8066f01494889541d7a997fbdc8","453fd7a5ddbe4c21aa087704d3ec2428","f76b9bcb963747fd983dc262c7d01285"]},"id":"5T2PdCu11GfG","executionInfo":{"status":"ok","timestamp":1718061221084,"user_tz":-180,"elapsed":9124,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}},"outputId":"7fb81c3b-cf21-4920-b640-10c10ae6c1ef"},"execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/1218 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f24513bc8a8d4fecb033fb8c87642730"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/1218 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e74a91be03c544ca9fdb1667e901cace"}},"metadata":{}}]},{"cell_type":"code","source":["#### ignore this\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","#from transformers import BertTokenizer, BertForMultipleChoice\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","# Custom Dataset class\n","class MCQADataset(Dataset):\n","    def __init__(self, data):\n","        self.questions = data['question']\n","        self.answers = data['answer']\n","        self.answer_map = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}  # Example answer mapping\n","\n","    def __len__(self):\n","        return len(self.questions)\n","\n","    def __getitem__(self, idx):\n","        question = self.questions[idx]\n","        answer = self.answers[idx]\n","        answer_idx = self.answer_map[answer]  # Convert letter answer to index\n","        return question, answer_idx\n","\n","\n","# Create dataset and dataloader\n","data = MCQADataset(train_subset)\n","dataloader = DataLoader(data, batch_size=2, shuffle=True)\n","\n","# Define device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Example tokenizer and model\n","#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","#model = BertForMultipleChoice.from_pretrained('bert-base-uncased')\n","model_output_dir = \"./model_output_second\"\n","model = AutoModelForCausalLM.from_pretrained(model_output_dir)\n","\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_output_dir)\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","\n","model.to(device)  # Move model to the device\n","optimizer = torch.optim.Adam(model.parameters())\n","loss_function = torch.nn.CrossEntropyLoss()\n","\n","for questions, answers in dataloader:\n","    # Tokenize questions\n","    input_tensors = []\n","    attention_masks = []\n","    for q in questions:\n","        inputs = tokenizer(q, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=1024)\n","        input_tensors.append(inputs['input_ids'])\n","        attention_masks.append(inputs['attention_mask'])\n","\n","    # Stack inputs into a single tensor (batch_size, num_choices, sequence_length)\n","    input_tensors = torch.stack(input_tensors).to(device)  # Move to device\n","    attention_masks = torch.stack(attention_masks).to(device)  # Move to device\n","\n","    # Ensure answers are in tensor form and move to device\n","    answer_tensors = torch.tensor(answers).to(device)\n","    print(answer_tensors.shape)\n","\n","    # Forward pass\n","    outputs = model(input_ids=input_tensors, attention_mask=attention_masks)\n","    print(outputs.logits.shape)\n","\n","    # Compute loss\n","    loss = loss_function(outputs.logits, answer_tensors)\n","\n","    # Backpropagation and optimization steps\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":463},"id":"gMpt4b5-Fzq0","executionInfo":{"status":"error","timestamp":1718056200624,"user_tz":-180,"elapsed":3940,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}},"outputId":"73f27118-2d42-4d4c-f956-b43c905e7c65"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-12-8ac591633e88>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  answer_tensors = torch.tensor(answers).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([2])\n","torch.Size([2, 1, 1024, 50257])\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"only batches of spatial targets supported (3D tensors) but got targets of size: : [2]","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-8ac591633e88>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# Backpropagation and optimization steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1186\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m                                label_smoothing=self.label_smoothing)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3084\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3085\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3086\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: only batches of spatial targets supported (3D tensors) but got targets of size: : [2]"]}]},{"cell_type":"code","source":["#### ignore this\n","\n","import re\n","\n","class MCQModel:\n","    def __init__(self, generator_model):\n","        self.generator = generator_model\n","\n","    def generate_reasoning(self, question_with_options):\n","        \"\"\"\n","        This method uses the generator model to produce detailed reasoning for the MCQ.\n","        \"\"\"\n","        # Implement the logic to generate reasoning using the generator model.\n","        # Here, we assume the generator model has a method `generate` that takes a prompt and returns a reasoning.\n","        prompt = question_with_options + \"\\nAnswer with reasoning:\"\n","        reasoning = self.generator.generate(prompt)\n","        return reasoning\n","\n","    def extract_answer(self, reasoning):\n","        \"\"\"\n","        This post-processing function extracts the single letter answer from the generated reasoning.\n","        \"\"\"\n","        # Implement logic to extract the final answer option from the reasoning.\n","        # For simplicity, let's assume the answer is mentioned at the end of the reasoning.\n","        match = re.search(r'Answer:\\s*([A-E])', reasoning)\n","        if match:\n","            return match.group(1)\n","        else:\n","            # Fallback to first letter detected if specific pattern is not found.\n","            for option in ['A', 'B', 'C', 'D', 'E']:\n","                if option in reasoning:\n","                    return option\n","        return None\n","\n","    def prediction_step_mcqa(self, question, options):\n","        \"\"\"\n","        This function performs the prediction step for MCQ answering.\n","        \"\"\"\n","        question_with_options = f\"Question: {question}\\n\\nOptions:\\n{options}\\nAnswer:\"\n","        reasoning = self.generate_reasoning(question_with_options)\n","        answer = self.extract_answer(reasoning)\n","        return answer\n","\n","\n","\n","model_output_dir = \"./model_output_second\"\n","#model = AutoModelForCausalLM.from_pretrained(model_output_dir)\n","model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n","\n","#mcq_model = MCQModel(generator_model)\n","\n","def generate_response(prompt, model, tokenizer, max_length=50):\n","    # Tokenize the input prompt with attention mask\n","    inputs = tokenizer.encode(prompt, return_tensors='pt', padding=True, truncation=True)\n","    attention_mask = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True)['attention_mask']\n","\n","    # Generate response from GPT-2\n","    outputs = model.generate(\n","        inputs,\n","        attention_mask=attention_mask,\n","        max_length=max_length,\n","        num_return_sequences=1,\n","        no_repeat_ngram_size=2,\n","        early_stopping=True,\n","        pad_token_id=tokenizer.eos_token_id  # Explicitly set pad token ID to EOS token ID\n","    )\n","\n","    # Decode the generated tokens to text\n","    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    return response\n","\n","#question = \"Question: what is the sum of the greatest common factor and the lowest common multiple of 36 and 56 ?\"\n","#options = \"A:22\\nB:24\\nC:26\\nD:28\\nE:30\"\n","#prompt_text = question\n","#answer = model(question + options)\n","\n","# Prompt to send to GPT-2\n","prompt = \"What is the capital of France? A) Paris; B) Rome; C) London; D) Madrid.\"\n","\n","# Get the response from GPT-2\n","response = generate_response(prompt, model, tokenizer)\n","\n","# Print the response\n","print(response)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XCGEwrADOimf","executionInfo":{"status":"ok","timestamp":1718060472924,"user_tz":-180,"elapsed":6172,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}},"outputId":"9ec2f36c-cc9f-479d-f2d7-0d61f9c6c94d"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["What is the capital of France? A) Paris; B) Rome; C) London; D) Madrid.\n","\n","The capital is Paris, and the city is called the \"Parisian capital.\" The capital was founded in 1789, when\n"]}]},{"cell_type":"code","source":["#### ignore this\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n","\n","class MCQADataset(Dataset):\n","    def __init__(self, data, tokenizer):\n","        self.questions = data['question']\n","        self.answers = data['answer']\n","        self.answer_map = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}  # Example answer mapping\n","        self.tokenizer = tokenizer\n","\n","    def __len__(self):\n","        return len(self.questions)\n","\n","    def __getitem__(self, idx):\n","        question = self.questions[idx]\n","        answer = self.answers[idx]\n","        answer_idx = self.answer_map[answer]  # Convert letter answer to index\n","\n","        # Parse the question and options\n","        question_text, options_text = question.split(\"\\n\\nOptions:\\n\")\n","        choices = options_text.split(\"\\n\")[1:]  # Split the options by newline and skip the first line\n","\n","        # Tokenize the input text for each choice\n","        input_ids = [self.tokenizer(f\"{question_text} {choice}\", return_tensors=\"pt\", padding='max_length', max_length=512, truncation=True).input_ids.squeeze(0) for choice in choices]\n","\n","        # Stack input_ids\n","        input_ids = torch.stack(input_ids, dim=0)\n","\n","        return input_ids, answer_idx\n","\n","\n","\n","# Tokenizer and dataset\n","model_output_dir = \"./model_output_second\"\n","model = AutoModelForCausalLM.from_pretrained(model_output_dir)\n","tokenizer = AutoTokenizer.from_pretrained(model_output_dir)\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","dataset = MCQADataset(train_subset, tokenizer)\n","dataloader = DataLoader(dataset, batch_size=1, shuffle=True)  # Adjust batch_size as needed\n","\n","# Define device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load GPT-2 model fine-tuned for sequence classification\n","#model = GPT2ForSequenceClassification.from_pretrained('path_to_your_fine_tuned_gpt2')\n","model.to(device)\n","optimizer = torch.optim.Adam(model.parameters())\n","loss_function = torch.nn.CrossEntropyLoss()\n","\n","for input_ids_batch, answers in dataloader:\n","    # Move input tensors and target tensors to device\n","    input_ids_batch = input_ids_batch.to(device)  # Shape: (batch_size, num_choices, sequence_length)\n","    answers = answers.to(device)  # Shape: (batch_size)\n","\n","    # Reshape input_ids_batch to (batch_size * num_choices, sequence_length)\n","    input_ids_batch = input_ids_batch.view(-1, input_ids_batch.size(-1))\n","\n","    expanded_answers = answers.repeat_interleave(5)  # Each question has 5 choices\n","\n","    # Forward pass\n","    outputs = model(input_ids=input_ids_batch, labels=expanded_answers)\n","\n","\n","    # The model's outputs include loss if labels are provided\n","    loss = outputs.loss\n","\n","    # Backpropagation and optimization steps\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    print(f\"Loss: {loss.item()}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"FY4DlP6kJ-Ve","executionInfo":{"status":"error","timestamp":1718056965174,"user_tz":-180,"elapsed":3572,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}},"outputId":"c38dc39b-e7bc-472d-8bae-97e30da1ff8d"},"execution_count":14,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Expected input batch_size (2555) to match target batch_size (4).","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-120ef1005ad6>\u001b[0m in \u001b[0;36m<cell line: 52>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpanded_answers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1333\u001b[0m             \u001b[0;31m# Flatten the tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m             \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshift_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1186\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m                                label_smoothing=self.label_smoothing)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3084\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3085\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3086\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Expected input batch_size (2555) to match target batch_size (4)."]}]},{"cell_type":"code","source":["#### ignore this\n","\n","import torch\n","from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n","from peft import LoraConfig, get_peft_model\n","\n","# LoRA configuration\n","peft_config = LoraConfig(\n","    r=16,\n","    lora_alpha=16,\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n","    target_modules=['c_attn', 'c_proj', 'mlp.c_fc', 'mlp.c_proj']  # Targeting GPT-2 specific modules\n",")\n","\n","# Load the model\n","model = AutoModelForCausalLM.from_pretrained(model_output_dir)\n","model = get_peft_model(model, peft_config)\n","\n","# Training arguments for the first fine-tuning\n","training_args = TrainingArguments(\n","    per_device_train_batch_size=4,\n","    gradient_accumulation_steps=4,\n","    learning_rate=5e-5,\n","    lr_scheduler_type=\"cosine\",\n","    max_steps=200,\n","    save_strategy=\"no\",\n","    logging_steps=10,\n","    output_dir=\"./model_output_mcqa_1\",\n","    optim=\"adamw_torch\",\n",")\n","\n","# Trainer setup for the first fine-tuning\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=validation_dataset,\n","    tokenizer=tokenizer,\n",")\n","\n","# Train the model on the first dataset\n","trainer.train()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":451},"id":"P3p3CVPX10MG","executionInfo":{"status":"error","timestamp":1718061285553,"user_tz":-180,"elapsed":53022,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}},"outputId":"4ec4ddcc-6fec-4684-bb89-f148f609d4de"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","max_steps is given, it will override any value given in num_train_epochs\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='11' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 11/200 00:43 < 15:20, 0.21 it/s, Epoch 0.13/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-37978faaddc4>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Train the model on the first dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1884\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1885\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1886\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1887\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2215\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2216\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2218\u001b[0m                 if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3240\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3241\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3243\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py\u001b[0m in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m     \"\"\"\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_emptyCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["#### Continue here\n","#### The idea is to customize a loss function for the MCQA data training:\n","#### If we just send the prompt (MCQ) to the model,\n","#### we assume it's able to generate an answer, but not in the right format\n","#### for example, with the MCQ \"What is the biggest? A)2; B)4; C)6; D)8\"\n","#### we assume the model generates an answer like \"8 is the biggest\"\n","#### but for MCQA we hope it generates \"D\"\n","#### so we apply a post-process function that compares the genrated answer like \"8 is the biggest\" with all the options (2, 4, 6, 8)\n","#### and choose the closest option (8) and take that choice (D) as the \"final output\"\n","#### so the new loss function would be to compare the label/target (D) and the \"final output\"\n","\n","import json\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n","import torch\n","\n","\n","# Initialize the tokenizer and model\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","tokenizer.pad_token = tokenizer.eos_token\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","\n","# Tokenize the data\n","def tokenize_function(example):\n","    input_text = example[\"question\"]\n","    target_text = example[\"answer\"]\n","\n","    # Extracting choices\n","    choices = [line.split(\":\")[1] for line in input_text.split(\"\\n\") if line.startswith(('A:', 'B:', 'C:', 'D:', 'E:'))]\n","    choice_labels = [line.split(\":\")[0] for line in input_text.split(\"\\n\") if line.startswith(('A:', 'B:', 'C:', 'D:', 'E:'))]\n","\n","    inputs = tokenizer(input_text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n","    targets = tokenizer(target_text, padding=\"max_length\", truncation=True, max_length=4, return_tensors=\"pt\")\n","\n","    return {\n","        \"input_ids\": inputs[\"input_ids\"].squeeze(),\n","        \"labels\": targets[\"input_ids\"].squeeze(),\n","        \"choices\": choices,\n","        \"choice_labels\": choice_labels\n","    }\n","\n","# Tokenize the dataset\n","tokenized_datasets = [tokenize_function(item) for item in train_subset]\n","\n","# Custom dataset class\n","class MCQADataset(torch.utils.data.Dataset):\n","    def __init__(self, tokenized_datasets):\n","        self.data = tokenized_datasets\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return self.data[idx]\n","\n","# Create the dataset\n","train_dataset = MCQADataset(tokenized_datasets)\n","\n","# Inspect the first sample\n","sample = train_dataset[0]\n","print(sample)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lLBKctjKdaI2","executionInfo":{"status":"ok","timestamp":1718063664407,"user_tz":-180,"elapsed":4051,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}},"outputId":"67713304-b36b-455e-a16f-7f88ccc8ae13"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': tensor([24361,    25,  1318,   389,   374,  2266,  2613,    11,   275,  4171,\n","         2613,   290,   266,  2330,  2613,   287,   257,  6131,    13,  1867,\n","          318,   262,  8064,   286,   262,  1271,   286,  4171,  2613,   284,\n","          262,  2472,   645,    13,   286,  2613,   287,  2846,   286,   374,\n","           11,   275,   290,   266, 40791,   198,   198, 29046,    25,   198,\n","           32,    25,    81,  1220,   357,    81,  1343,   275,  1343,   266,\n","            8,   198,    33,    25,    81,  1635,   357,    81,  1343,   275,\n","         1343,   266,     8,   198,    34, 37498,    81,  1343,   275,  1343,\n","          266,     8,   198,    35,    25,    81,  1220,   357,    81,  1343,\n","          275,  1267,   198,    36,    25,    81,  1220,   357,    65,  1343,\n","          266,     8,   198, 33706,    25, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]), 'labels': tensor([   32, 50256, 50256, 50256]), 'choices': ['r / (r + b + w)', 'r * (r + b + w)', '(r + b + w)', 'r / (r + b )', 'r / (b + w)'], 'choice_labels': ['A', 'B', 'C', 'D', 'E']}\n"]}]},{"cell_type":"code","source":["#### this collator was meant to ensure that columns \"choices\" and \"choice labels\" are kept\n","from transformers import DataCollatorWithPadding\n","\n","class DataCollatorForMCQA(DataCollatorWithPadding):\n","    def __call__(self, features):\n","        batch = super().__call__(features)\n","        batch[\"choices\"] = [feature[\"choices\"] for feature in features]\n","        batch[\"choice_labels\"] = [feature[\"choice_labels\"] for feature in features]\n","        return batch\n","data_collator = DataCollatorForMCQA(tokenizer)"],"metadata":{"id":"1wxPbXJfnkpr","executionInfo":{"status":"ok","timestamp":1718064887413,"user_tz":-180,"elapsed":339,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}}},"execution_count":82,"outputs":[]},{"cell_type":"code","source":["#### we might also need a better way to find the best match\n","def post_process_answer(generated_answer, choices):\n","    best_choice = None\n","    best_score = float('inf')\n","    for choice in choices:\n","        score = levenshtein_distance(generated_answer, choice)\n","        if score < best_score:\n","            best_score = score\n","            best_choice = choice\n","    return best_choice\n","\n","def levenshtein_distance(a, b):\n","    if len(a) < len(b):\n","        return levenshtein_distance(b, a)\n","\n","    if len(b) == 0:\n","        return len(a)\n","\n","    previous_row = range(len(b) + 1)\n","    for i, c1 in enumerate(a):\n","        current_row = [i + 1]\n","        for j, c2 in enumerate(b):\n","            insertions = previous_row[j + 1] + 1\n","            deletions = current_row[j] + 1\n","            substitutions = previous_row[j] + (c1 != c2)\n","            current_row.append(min(insertions, deletions, substitutions))\n","        previous_row = current_row\n","\n","    return previous_row[-1]\n"],"metadata":{"id":"amnJDboOdzca","executionInfo":{"status":"ok","timestamp":1718063719208,"user_tz":-180,"elapsed":284,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["# Custom loss function\n","def custom_loss_function(logits, labels, choices, choice_labels):\n","    # Generate the most probable sequence\n","    generated_ids = torch.argmax(logits, dim=-1)\n","    generated_answers = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n","\n","    # Post-process the generated answers\n","    post_processed_answers = [post_process_answer(answer, choice) for answer, choice in zip(generated_answers, choices)]\n","\n","    # Convert post-processed answers to choice labels\n","    post_processed_choice_labels = [choice_labels[choices.index(answer)] for answer, choices in zip(post_processed_answers, choices)]\n","\n","    # Calculate the loss with respect to the correct choice\n","    correct_choice_labels = [choice_labels.index(labels[i]) for i in range(len(labels))]\n","    post_processed_indices = [choice_labels.index(label) for label in post_processed_choice_labels]\n","\n","    loss = F.cross_entropy(torch.tensor(post_processed_indices), torch.tensor(correct_choice_labels))\n","\n","    return loss\n"],"metadata":{"id":"3ZMkKpEheOIN","executionInfo":{"status":"ok","timestamp":1718064464187,"user_tz":-180,"elapsed":305,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}}},"execution_count":74,"outputs":[]},{"cell_type":"code","source":["#### now the problem is, as shown by the print of \"inputs\", it doesn't contain \"labels\"\n","# Custom Trainer class\n","class CustomTrainer(Trainer):\n","    print(inputs)\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        print(inputs)\n","        labels = inputs.pop(\"labels\")\n","        choices = inputs.pop(\"choices\")\n","        choice_labels = inputs.pop(\"choice_labels\")\n","\n","        # Forward pass\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","\n","        # Custom loss computation\n","        loss = custom_loss_function(logits, labels, choices, choice_labels)\n","\n","        return (loss, outputs) if return_outputs else loss\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e9UG0DQ6eTY1","executionInfo":{"status":"ok","timestamp":1718064701411,"user_tz":-180,"elapsed":371,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}},"outputId":"f5a1d45a-cb62-4f06-b1d9-6e30ac6fd34b"},"execution_count":81,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[24361,    25,   644,   318,   262,  2160,   286,   262,  6000,  2219,\n","          5766,   290,   262,  9016,  2219,  3294,   286,  4570,   290,  7265,\n","          5633]])\n"]}]},{"cell_type":"code","source":["#### as long as the function \"compute_loss\" works, which means we can access columns \"labels\" in the train_dataset\n","# Training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n",")\n","print(train_dataset[0])\n","# Initialize the custom trainer\n","trainer = CustomTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    data_collator=data_collator\n",")\n","\n","# Start training\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":602},"id":"tac0zGlJn-yB","executionInfo":{"status":"error","timestamp":1718065792557,"user_tz":-180,"elapsed":1117,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}},"outputId":"5a4f0019-df28-41d6-a930-7ba26a358db5"},"execution_count":84,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["{'input_ids': tensor([24361,    25,  1318,   389,   374,  2266,  2613,    11,   275,  4171,\n","         2613,   290,   266,  2330,  2613,   287,   257,  6131,    13,  1867,\n","          318,   262,  8064,   286,   262,  1271,   286,  4171,  2613,   284,\n","          262,  2472,   645,    13,   286,  2613,   287,  2846,   286,   374,\n","           11,   275,   290,   266, 40791,   198,   198, 29046,    25,   198,\n","           32,    25,    81,  1220,   357,    81,  1343,   275,  1343,   266,\n","            8,   198,    33,    25,    81,  1635,   357,    81,  1343,   275,\n","         1343,   266,     8,   198,    34, 37498,    81,  1343,   275,  1343,\n","          266,     8,   198,    35,    25,    81,  1220,   357,    81,  1343,\n","          275,  1267,   198,    36,    25,    81,  1220,   357,    65,  1343,\n","          266,     8,   198, 33706,    25, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]), 'labels': tensor([   32, 50256, 50256, 50256]), 'choices': ['r / (r + b + w)', 'r * (r + b + w)', '(r + b + w)', 'r / (r + b )', 'r / (b + w)'], 'choice_labels': ['A', 'B', 'C', 'D', 'E']}\n"]},{"output_type":"error","ename":"KeyError","evalue":"'choices'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-84-384ad3a45218>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1884\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1885\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1886\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1887\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2177\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2178\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2179\u001b[0m                 \u001b[0mtotal_batched_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;31m# We iterate one batch ahead to check when we are at the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m             \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_remove_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-82-02139c0fa59b>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"choices\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"choices\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"choice_labels\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"choice_labels\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-82-02139c0fa59b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"choices\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"choices\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"choice_labels\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"choice_labels\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'choices'"]}]}]}