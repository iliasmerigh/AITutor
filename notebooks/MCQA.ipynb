{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyM0JzbN25JmZmyOxpIAXBD+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"f24513bc8a8d4fecb033fb8c87642730":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c9f8cc96491f4e4ab01aef4f4c86480e","IPY_MODEL_3f075aea1b234aeeb966b9f2f154fa3f","IPY_MODEL_b3ce3a6ffcfe4e36a016681fb8184bd7"],"layout":"IPY_MODEL_6815d005925b4c9f82d91edf770d7849"}},"c9f8cc96491f4e4ab01aef4f4c86480e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d585b44d142540b78e2784a5f0068b74","placeholder":"â€‹","style":"IPY_MODEL_58983789d4814cbf9ec99cee126bd7a8","value":"Map:â€‡100%"}},"3f075aea1b234aeeb966b9f2f154fa3f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_30ead5a037af44bfa8fde541ef15e110","max":1218,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4c645ee6e5854b49b902747ecc3ed86d","value":1218}},"b3ce3a6ffcfe4e36a016681fb8184bd7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5da0a8663d09409eb3678f995a25b8b6","placeholder":"â€‹","style":"IPY_MODEL_2d4c3d50c37f48488328efd929802c56","value":"â€‡1218/1218â€‡[00:05&lt;00:00,â€‡141.10â€‡examples/s]"}},"6815d005925b4c9f82d91edf770d7849":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d585b44d142540b78e2784a5f0068b74":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58983789d4814cbf9ec99cee126bd7a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"30ead5a037af44bfa8fde541ef15e110":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c645ee6e5854b49b902747ecc3ed86d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5da0a8663d09409eb3678f995a25b8b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d4c3d50c37f48488328efd929802c56":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e74a91be03c544ca9fdb1667e901cace":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c54354c1203a44b8b94c084ba4255bfa","IPY_MODEL_9e592fefb5144192a951ca54f3ed98e8","IPY_MODEL_2445c7ec787f469cac196e2013905a4d"],"layout":"IPY_MODEL_935aa0c468a64fb29cf5b1ffed009733"}},"c54354c1203a44b8b94c084ba4255bfa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3bd10c38a92a45c3b5302e85a06fb61a","placeholder":"â€‹","style":"IPY_MODEL_17b727a8691a473997dad506c686cdd8","value":"Map:â€‡100%"}},"9e592fefb5144192a951ca54f3ed98e8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e01e8d8ae0e4497ca690634475b62300","max":1218,"min":0,"orientation":"horizontal","style":"IPY_MODEL_de07a8066f01494889541d7a997fbdc8","value":1218}},"2445c7ec787f469cac196e2013905a4d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_453fd7a5ddbe4c21aa087704d3ec2428","placeholder":"â€‹","style":"IPY_MODEL_f76b9bcb963747fd983dc262c7d01285","value":"â€‡1218/1218â€‡[00:03&lt;00:00,â€‡487.22â€‡examples/s]"}},"935aa0c468a64fb29cf5b1ffed009733":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bd10c38a92a45c3b5302e85a06fb61a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17b727a8691a473997dad506c686cdd8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e01e8d8ae0e4497ca690634475b62300":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de07a8066f01494889541d7a997fbdc8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"453fd7a5ddbe4c21aa087704d3ec2428":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f76b9bcb963747fd983dc262c7d01285":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AItcIQ4AvD0p","executionInfo":{"status":"ok","timestamp":1718117937373,"user_tz":-180,"elapsed":7434,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}},"outputId":"1ac00429-3a89-4c17-dce1-a9d5973c8903"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n","Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.11.1)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.2)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.31.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.3.0+cu121)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.5.40)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"]}],"source":["!pip install transformers peft datasets accelerate -U"]},{"cell_type":"code","source":["# This cell makes sure modules are auto-loaded when you change external python files\n","%load_ext autoreload\n","%autoreload 2"],"metadata":{"id":"_DCzsMMawVsy","executionInfo":{"status":"ok","timestamp":1718117942398,"user_tz":-180,"elapsed":538,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Step 1: Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hd80oVtOwWrS","executionInfo":{"status":"ok","timestamp":1718117947889,"user_tz":-180,"elapsed":2745,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}},"outputId":"23813607-7cc8-4a75-d933-a4efaecbbcec"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Direct to your assignment folder.\n","%cd /content/drive/MyDrive/project-m3-2024-jim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z--8o_c9wYe6","executionInfo":{"status":"ok","timestamp":1718117950016,"user_tz":-180,"elapsed":5,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}},"outputId":"34bf0a50-d9df-4736-93ad-b4151e9d58e6"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/project-m3-2024-jim\n"]}]},{"cell_type":"code","source":["import os\n","\n","# Set the environment variable\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"],"metadata":{"id":"UbiOGLAtyjNc","executionInfo":{"status":"ok","timestamp":1718117952423,"user_tz":-180,"elapsed":478,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","dataset = load_dataset('json', data_files='datasets/mcq_aquarat_dataset.jsonl')\n","\n","# Print dataset information\n","print(dataset)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fwF3PmPBwbu0","executionInfo":{"status":"ok","timestamp":1718117957088,"user_tz":-180,"elapsed":2351,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}},"outputId":"a3af0ae0-cd93-412a-bd15-0ca8a01c88d6"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['subject', 'question', 'answer'],\n","        num_rows: 82846\n","    })\n","})\n"]}]},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","\n","# Shuffle the entire training dataset\n","shuffled_dataset = dataset['train'].shuffle(seed=42)\n","\n","# Define the size for the subsets (e.g., 10% for training, 10% for validation)\n","subset_size = len(dataset['train']) // 60\n","\n","# Generate the training subset (first 10%)\n","train_subset = shuffled_dataset.select(range(0, subset_size))\n","\n","# Generate the validation subset (next 10%)\n","validation_subset = shuffled_dataset.select(range(subset_size, 2 * subset_size))\n","\n","# Print the subset information\n","print(\"Training Subset:\")\n","print(train_subset)\n","\n","print(\"\\nValidation Subset:\")\n","print(validation_subset)\n","\n","#### Having a small training dataset and validation dataset\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3E0N1n1106c9","executionInfo":{"status":"ok","timestamp":1718121942148,"user_tz":-180,"elapsed":525,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}},"outputId":"76bb93f2-035e-4f9c-a5c7-cbefdf999e15"},"execution_count":84,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Subset:\n","Dataset({\n","    features: ['subject', 'question', 'answer'],\n","    num_rows: 1380\n","})\n","\n","Validation Subset:\n","Dataset({\n","    features: ['subject', 'question', 'answer'],\n","    num_rows: 1380\n","})\n"]}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["f24513bc8a8d4fecb033fb8c87642730","c9f8cc96491f4e4ab01aef4f4c86480e","3f075aea1b234aeeb966b9f2f154fa3f","b3ce3a6ffcfe4e36a016681fb8184bd7","6815d005925b4c9f82d91edf770d7849","d585b44d142540b78e2784a5f0068b74","58983789d4814cbf9ec99cee126bd7a8","30ead5a037af44bfa8fde541ef15e110","4c645ee6e5854b49b902747ecc3ed86d","5da0a8663d09409eb3678f995a25b8b6","2d4c3d50c37f48488328efd929802c56","e74a91be03c544ca9fdb1667e901cace","c54354c1203a44b8b94c084ba4255bfa","9e592fefb5144192a951ca54f3ed98e8","2445c7ec787f469cac196e2013905a4d","935aa0c468a64fb29cf5b1ffed009733","3bd10c38a92a45c3b5302e85a06fb61a","17b727a8691a473997dad506c686cdd8","e01e8d8ae0e4497ca690634475b62300","de07a8066f01494889541d7a997fbdc8","453fd7a5ddbe4c21aa087704d3ec2428","f76b9bcb963747fd983dc262c7d01285"]},"id":"5T2PdCu11GfG","executionInfo":{"status":"ok","timestamp":1718061221084,"user_tz":-180,"elapsed":9124,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}},"outputId":"7fb81c3b-cf21-4920-b640-10c10ae6c1ef"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/1218 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f24513bc8a8d4fecb033fb8c87642730"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/1218 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e74a91be03c544ca9fdb1667e901cace"}},"metadata":{}}]},{"cell_type":"code","source":["#### Continue here\n","#### The idea is to customize a loss function for the MCQA data training:\n","#### If we just send the prompt (MCQ) to the model,\n","#### we assume it's able to generate an answer, but not in the right format\n","#### for example, with the MCQ \"What is the biggest? A)2; B)4; C)6; D)8\"\n","#### we assume the model generates an answer like \"8 is the biggest\"\n","#### but for MCQA we hope it generates \"D\"\n","#### so we apply a post-process function that compares the genrated answer like \"8 is the biggest\" with all the options (2, 4, 6, 8)\n","#### and choose the closest option (8) and take that choice (D) as the \"final output\"\n","#### so the new loss function would be to compare the label/target (D) and the \"final output\"\n","\n","import json\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n","import torch\n","from torch.utils.data import Dataset\n","from sklearn.preprocessing import LabelEncoder\n","\n","\n","# Initialize the tokenizer and model\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","tokenizer.pad_token = tokenizer.eos_token\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","\n","\n","# Initialize LabelEncoder and fit it with the possible choice labels\n","label_encoder = LabelEncoder()\n","label_encoder.fit(['A', 'B', 'C', 'D', 'E'])\n","\n","def tokenize_function(example):\n","    input_text = example[\"question\"]\n","    target_text = example[\"answer\"]\n","\n","    # Extracting choices and their labels\n","    choices = [line.split(\":\")[1].strip() for line in input_text.split(\"\\n\") if line.startswith(('A:', 'B:', 'C:', 'D:', 'E:'))]\n","    choice_labels = [line.split(\":\")[0] for line in input_text.split(\"\\n\") if line.startswith(('A:', 'B:', 'C:', 'D:', 'E:'))]\n","\n","    # Tokenizing input text\n","    inputs = tokenizer(input_text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n","\n","    # Encode the target text (single letter choice)\n","    target_label = label_encoder.transform([target_text])[0]\n","\n","    # Encode choice labels to integers\n","    encoded_choice_labels = label_encoder.transform(choice_labels)\n","\n","    return {\n","        \"input_ids\": inputs[\"input_ids\"].squeeze(),\n","        \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n","        \"labels\": torch.tensor(target_label),\n","        \"choices\": choices,\n","        \"choice_labels\": torch.tensor(encoded_choice_labels)\n","    }\n","\n","# Tokenize the dataset\n","tokenized_datasets_train = [tokenize_function(item) for item in train_subset]\n","tokenized_datasets_eval = [tokenize_function(item) for item in validation_subset]\n","\n","\n","class MCQADataset(Dataset):\n","    def __init__(self, data):\n","        self.data = data\n","\n","    def __getitem__(self, idx):\n","        item = {\n","            'input_ids': torch.tensor(self.data[idx]['input_ids']),\n","            'attention_mask': torch.tensor(self.data[idx]['attention_mask']),\n","            'labels': torch.tensor(self.data[idx]['labels']),\n","            'choices': self.data[idx]['choices'],\n","            'choice_labels': self.data[idx]['choice_labels']\n","        }\n","        return item\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","\n","\n","# Create the dataset\n","train_dataset = MCQADataset(tokenized_datasets_train)\n","\n","eval_dataset = MCQADataset(tokenized_datasets_eval)\n","\n","# Inspect the first sample\n","sample = train_dataset[4]\n","print(sample)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lLBKctjKdaI2","executionInfo":{"status":"ok","timestamp":1718121956975,"user_tz":-180,"elapsed":4619,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}},"outputId":"dacca00e-a9b2-4748-f90e-e85973b6e655"},"execution_count":85,"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': tensor([24361,    25,  1002,   281,  2708,   318,  2702,   379,  1160,     4,\n","         7630,  2427,   286,  1478,     4,  7630,    11,   788,   262,  7630,\n","          561,   307, 12820,    13, 11546,   517,    13,  1867,   318,   262,\n","         1575,  2756,    30,   198,   198, 29046,    25,   198,    32,    25,\n","        31273,    13, 12825,   198,    33,    25, 31273,    13,  4751,   198,\n","           34,    25, 31273,    13, 20343,   198,    35,    25, 31273,    13,\n","        30123,   198,    36,    25, 31273,    13,    20,  2167,   198, 33706,\n","           25, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(2), 'choices': ['Rs.1000', 'Rs. 2000', 'Rs. 3000', 'Rs. 4000', 'Rs.5200'], 'choice_labels': tensor([0, 1, 2, 3, 4])}\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-85-399306e68457>:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  'input_ids': torch.tensor(self.data[idx]['input_ids']),\n","<ipython-input-85-399306e68457>:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  'attention_mask': torch.tensor(self.data[idx]['attention_mask']),\n","<ipython-input-85-399306e68457>:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  'labels': torch.tensor(self.data[idx]['labels']),\n"]}]},{"cell_type":"code","source":["#### this collator was meant to ensure that columns \"choices\" and \"choice labels\" are kept\n","from transformers import DataCollatorWithPadding\n","\n","class CustomDataCollator(DataCollatorWithPadding):\n","    def __call__(self, features):\n","        # Extract the special fields\n","        choices = [feature.pop('choices') for feature in features]\n","        choice_labels = [feature.pop('choice_labels') for feature in features]\n","\n","        # Use the parent class's __call__ method to handle the rest\n","        batch = super().__call__(features)\n","\n","        # Add the special fields back to the batch\n","        batch['choices'] = choices\n","        batch['choice_labels'] = choice_labels\n","\n","        return batch\n","\n","data_collator = CustomDataCollator(tokenizer)"],"metadata":{"id":"1wxPbXJfnkpr","executionInfo":{"status":"ok","timestamp":1718121960348,"user_tz":-180,"elapsed":538,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}}},"execution_count":86,"outputs":[]},{"cell_type":"code","source":["\n","def levenshtein_distance(a, b):\n","    if len(a) < len(b):\n","        return levenshtein_distance(b, a)\n","\n","    if len(b) == 0:\n","        return len(a)\n","\n","    previous_row = range(len(b) + 1)\n","    for i, c1 in enumerate(a):\n","        current_row = [i + 1]\n","        for j, c2 in enumerate(b):\n","            insertions = previous_row[j + 1] + 1\n","            deletions = current_row[j] + 1\n","            substitutions = previous_row[j] + (c1 != c2)\n","            current_row.append(min(insertions, deletions, substitutions))\n","        previous_row = current_row\n","\n","    return previous_row[-1]\n"],"metadata":{"id":"amnJDboOdzca","executionInfo":{"status":"ok","timestamp":1718121962842,"user_tz":-180,"elapsed":459,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}}},"execution_count":87,"outputs":[]},{"cell_type":"code","source":["import torch.nn.functional as F\n","\n","def custom_loss_function(logits, labels, choices, choice_labels, temperature=1.0):\n","    # Ensure labels are of type torch.long\n","    labels = labels.long()\n","\n","    # Generate the most probable sequence from logits\n","    generated_ids = torch.argmax(logits, dim=-1)\n","    generated_answers = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n","\n","    # Initialize a list to store the new logits\n","    new_logits_list = []\n","    valid_labels = []\n","\n","    # Post-process the generated answers to compute new logits\n","    for idx, (generated_answer, choice_list, label) in enumerate(zip(generated_answers, choices, labels)):\n","        if len(choice_list) != 5:\n","            #print(f\"Skipping entry because it does not have exactly 5 choices.\")\n","            continue\n","\n","        scores = []\n","        max_len = max(len(generated_answer), max(len(choice) for choice in choice_list))\n","        for choice in choice_list:\n","            score = levenshtein_distance(generated_answer, choice)\n","            normalized_score = score / max_len  # Normalize distance\n","            inverse_score = 1 / (normalized_score + 1e-5)  # Apply inverse transformation\n","            scores.append(inverse_score)\n","\n","        # Apply min-max scaling\n","        scores = torch.tensor(scores, device=logits.device, dtype=torch.float)\n","        min_score = scores.min()\n","        max_score = scores.max()\n","        epsilon = 1e-5  # Small value to prevent division by zero\n","        scaled_scores = (scores - min_score) / (max_score - min_score + epsilon)\n","\n","        # Apply temperature-scaled softmax\n","        exp_scores = torch.exp(scaled_scores / temperature)\n","        softmax_scores = exp_scores / exp_scores.sum()\n","        new_logits_list.append(softmax_scores)\n","        valid_labels.append(label)\n","\n","    # Convert the list of logits and labels to tensors\n","    if new_logits_list:\n","        new_logits = torch.stack(new_logits_list).requires_grad_(True)  # Ensure gradient tracking\n","        valid_labels = torch.tensor(valid_labels).long().to(logits.device)\n","\n","        # Debugging: Print tensor shapes and values\n","        #print(f\"Logits shape: {logits.shape}\")\n","        #print(f\"New logits shape: {new_logits.shape}\")\n","        #print(f\"Labels: {valid_labels}\")\n","        #print(f\"New logits: {new_logits}\")\n","\n","        # Ensure labels are within the valid range\n","        if valid_labels.max() >= new_logits.size(1) or valid_labels.min() < 0:\n","            raise ValueError(f\"Labels out of range: {valid_labels}\")\n","\n","        # Calculate cross-entropy loss\n","        loss = F.cross_entropy(new_logits, valid_labels)\n","\n","    # Calculate cross-entropy loss\n","    #loss = F.cross_entropy(new_logits, labels)\n","\n","    return loss"],"metadata":{"id":"3ZMkKpEheOIN","executionInfo":{"status":"ok","timestamp":1718121965788,"user_tz":-180,"elapsed":498,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}}},"execution_count":88,"outputs":[]},{"cell_type":"code","source":["#### now the problem is, as shown by the print of \"inputs\", it doesn't contain \"labels\"\n","# Custom Trainer class\n","from transformers import Trainer, TrainingArguments, BertForSequenceClassification\n","from torch.utils.data import DataLoader, SequentialSampler\n","\n","class CustomTrainer(Trainer):\n","    def get_train_dataloader(self):\n","        train_sampler = self._get_train_sampler()\n","        return DataLoader(\n","            self.train_dataset,\n","            batch_size=self.args.train_batch_size,\n","            sampler=train_sampler,\n","            collate_fn=self.data_collator,\n","        )\n","\n","    def get_eval_dataloader(self, eval_dataset=None):\n","        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n","        return DataLoader(\n","            eval_dataset,\n","            batch_size=self.args.eval_batch_size,\n","            sampler=SequentialSampler(eval_dataset),\n","            collate_fn=self.data_collator,\n","        )\n","    #print(inputs)\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        #print(inputs)\n","        input_ids = inputs.pop(\"input_ids\")\n","        attention_mask = inputs.pop(\"attention_mask\")\n","        labels = inputs.pop(\"labels\")\n","        choices = inputs.pop(\"choices\")\n","        choice_labels = inputs.pop(\"choice_labels\")\n","\n","        # Forward pass\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","\n","        # Custom loss computation\n","        loss = custom_loss_function(logits, labels, choices, choice_labels)\n","\n","        return (loss, outputs) if return_outputs else loss\n"],"metadata":{"id":"e9UG0DQ6eTY1","executionInfo":{"status":"ok","timestamp":1718121969848,"user_tz":-180,"elapsed":543,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}}},"execution_count":89,"outputs":[]},{"cell_type":"code","source":["#### as long as the function \"compute_loss\" works, which means we can access columns \"labels\" in the train_dataset\n","# Training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    num_train_epochs=5,\n","    weight_decay=0.01,\n","    logging_dir=\"./logs\",\n","    logging_steps=10,  # Adjust logging steps as needed\n","    log_level='info',  # Ensure logging level includes info\n",")\n","#print(train_dataset[0])\n","# Initialize the custom trainer\n","trainer = CustomTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    data_collator=data_collator\n",")\n","\n","# Start training\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"tac0zGlJn-yB","executionInfo":{"status":"ok","timestamp":1718122360216,"user_tz":-180,"elapsed":304901,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}},"outputId":"548a80ee-ebb9-4a1c-f750-054eea83bb1b"},"execution_count":92,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1,380\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 865\n","  Number of trainable parameters = 124,439,808\n","<ipython-input-85-399306e68457>:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  'input_ids': torch.tensor(self.data[idx]['input_ids']),\n","<ipython-input-85-399306e68457>:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  'attention_mask': torch.tensor(self.data[idx]['attention_mask']),\n","<ipython-input-85-399306e68457>:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  'labels': torch.tensor(self.data[idx]['labels']),\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='865' max='865' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [865/865 05:03, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.599600</td>\n","      <td>1.609528</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>1.621600</td>\n","      <td>1.609528</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.598900</td>\n","      <td>1.609528</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>1.606200</td>\n","      <td>1.609528</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>1.610200</td>\n","      <td>1.609528</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1380\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 1380\n","  Batch size = 8\n","Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Configuration saved in ./results/checkpoint-500/generation_config.json\n","Model weights saved in ./results/checkpoint-500/model.safetensors\n","<ipython-input-85-399306e68457>:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  'input_ids': torch.tensor(self.data[idx]['input_ids']),\n","<ipython-input-85-399306e68457>:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  'attention_mask': torch.tensor(self.data[idx]['attention_mask']),\n","<ipython-input-85-399306e68457>:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  'labels': torch.tensor(self.data[idx]['labels']),\n","***** Running Evaluation *****\n","  Num examples = 1380\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 1380\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 1380\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=865, training_loss=1.6047728202246516, metrics={'train_runtime': 304.1112, 'train_samples_per_second': 22.689, 'train_steps_per_second': 2.844, 'total_flos': 450728755200000.0, 'train_loss': 1.6047728202246516, 'epoch': 5.0})"]},"metadata":{},"execution_count":92}]},{"cell_type":"code","source":["# Save the final model and tokenizer\n","model.save_pretrained(\"./MCQA_model_1\")\n","tokenizer.save_pretrained(\"./MCQA_model_1\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MLjImpeIFT6I","executionInfo":{"status":"ok","timestamp":1718122552959,"user_tz":-180,"elapsed":5038,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}},"outputId":"161e74a3-f5c2-4338-e04a-e549f127a413"},"execution_count":93,"outputs":[{"output_type":"stream","name":"stderr","text":["Configuration saved in ./MCQA_model_1/config.json\n","Configuration saved in ./MCQA_model_1/generation_config.json\n","Model weights saved in ./MCQA_model_1/model.safetensors\n","tokenizer config file saved in ./MCQA_model_1/tokenizer_config.json\n","Special tokens file saved in ./MCQA_model_1/special_tokens_map.json\n"]},{"output_type":"execute_result","data":{"text/plain":["('./MCQA_model_1/tokenizer_config.json',\n"," './MCQA_model_1/special_tokens_map.json',\n"," './MCQA_model_1/vocab.json',\n"," './MCQA_model_1/merges.txt',\n"," './MCQA_model_1/added_tokens.json')"]},"metadata":{},"execution_count":93}]}]}